{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본적인 이미지 데이터 셋인 mnist 데이터를 tensorflow로 single layer로 구성된 Softmax Classification을 구현하여 학습해보고자 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tenssorflow==2.3 환경에서 구현  \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(mnist_train, mnist_train_label), (mnist_test, mnist_test_label) = mnist.load_data() #mnist data road\n",
    "mnist_train = tf.reshape(mnist_train, shape=(len(mnist_train), 28*28)) #mnist train data reshape\n",
    "mnist_train=tf.cast(mnist_train, tf.float32)\n",
    "mnist_test = tf.reshape(mnist_test, shape=(len(mnist_test), 28*28)) #mnist test data reshape\n",
    "mnist_test=tf.cast(mnist_test, tf.float32)\n",
    "\n",
    "#one hot encoding\n",
    "train_labels = to_categorical(mnist_train_label, 10)\n",
    "test_labels = to_categorical(mnist_test_label, 10) \n",
    "\n",
    "class_number = 10  \n",
    "feature_number=28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 수와 피쳐 수에 따라서 weigh와 bias 변수 설정\n",
    "w = tf.Variable(tf.random.uniform([feature_number,class_number]))\n",
    "b = tf.Variable(tf.random.uniform([class_number]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss_value: 865.98914\n",
      "100 loss_value: 33.921616\n",
      "200 loss_value: 20.97104\n",
      "300 loss_value: 15.103278\n",
      "400 loss_value: 11.460324\n",
      "500 loss_value: 8.798818\n",
      "600 loss_value: 6.8483124\n",
      "700 loss_value: 5.501834\n",
      "800 loss_value: 4.53413\n",
      "900 loss_value: 3.7272825\n",
      "1000 loss_value: 3.5559633\n",
      "1100 loss_value: 4.5672903\n",
      "1200 loss_value: 5.399879\n",
      "1300 loss_value: 4.731648\n",
      "1400 loss_value: 13.499748\n",
      "1500 loss_value: 1.9843326\n",
      "1600 loss_value: 2.1227198\n",
      "1700 loss_value: 1.9097672\n",
      "1800 loss_value: 2.863239\n",
      "1900 loss_value: 3.1841471\n",
      "2000 loss_value: 9.48028\n",
      "2100 loss_value: 1.9917927\n",
      "2200 loss_value: 2.6315174\n",
      "2300 loss_value: 1.3579023\n",
      "2400 loss_value: 8.608444\n",
      "2500 loss_value: 3.1710434\n",
      "2600 loss_value: 2.123608\n",
      "2700 loss_value: 3.9466069\n",
      "2800 loss_value: 3.014698\n",
      "2900 loss_value: 1.594696\n",
      "3000 loss_value: 3.1451075\n",
      "3100 loss_value: 2.3207724\n",
      "3200 loss_value: 4.2211676\n",
      "3300 loss_value: 11.651646\n",
      "3400 loss_value: 1.3758498\n",
      "3500 loss_value: 2.8808725\n",
      "3600 loss_value: 3.1564534\n",
      "3700 loss_value: 6.6714416\n",
      "3800 loss_value: 5.2443514\n",
      "3900 loss_value: 2.8194551\n",
      "4000 loss_value: 1.1471338\n",
      "4100 loss_value: 2.178859\n",
      "4200 loss_value: 1.7656478\n",
      "4300 loss_value: 9.894406\n",
      "4400 loss_value: 1.731099\n",
      "4500 loss_value: 2.3219173\n",
      "4600 loss_value: 2.547929\n",
      "4700 loss_value: 2.296299\n",
      "4800 loss_value: 12.843494\n",
      "4900 loss_value: 2.2239208\n",
      "5000 loss_value: 1.3307524\n",
      "5100 loss_value: 2.2164578\n",
      "5200 loss_value: 1.8327279\n",
      "5300 loss_value: 4.8065658\n",
      "5400 loss_value: 12.573585\n",
      "5500 loss_value: 1.465727\n",
      "5600 loss_value: 1.831508\n",
      "5700 loss_value: 10.643273\n",
      "5800 loss_value: 1.2493384\n",
      "5900 loss_value: 1.1056384\n",
      "6000 loss_value: 2.393185\n",
      "6100 loss_value: 1.2929004\n",
      "6200 loss_value: 2.8860838\n",
      "6300 loss_value: 2.8526645\n",
      "6400 loss_value: 1.2383689\n",
      "6500 loss_value: 4.8645515\n",
      "6600 loss_value: 1.3457115\n",
      "6700 loss_value: 2.3458726\n",
      "6800 loss_value: 1.7083865\n",
      "6900 loss_value: 1.4146659\n",
      "7000 loss_value: 1.591376\n",
      "7100 loss_value: 1.5827261\n",
      "7200 loss_value: 2.5934389\n",
      "7300 loss_value: 2.351833\n",
      "7400 loss_value: 1.8945802\n",
      "7500 loss_value: 1.7618818\n",
      "7600 loss_value: 3.1200745\n",
      "7700 loss_value: 2.5796633\n",
      "7800 loss_value: 1.327186\n",
      "7900 loss_value: 1.8539556\n",
      "8000 loss_value: 4.438127\n",
      "8100 loss_value: 3.1478765\n",
      "8200 loss_value: 1.3482782\n",
      "8300 loss_value: 1.5703255\n",
      "8400 loss_value: 11.19154\n",
      "8500 loss_value: 1.702594\n",
      "8600 loss_value: 1.6793271\n",
      "8700 loss_value: 1.3741591\n",
      "8800 loss_value: 2.7102704\n",
      "8900 loss_value: 1.847704\n",
      "9000 loss_value: 2.1962495\n",
      "9100 loss_value: 2.0234904\n",
      "9200 loss_value: 1.7764119\n",
      "9300 loss_value: 1.7463475\n",
      "9400 loss_value: 1.3531122\n",
      "9500 loss_value: 13.5864\n",
      "9600 loss_value: 1.5455132\n",
      "9700 loss_value: 2.258561\n",
      "9800 loss_value: 1.439225\n",
      "9900 loss_value: 2.9136317\n"
     ]
    }
   ],
   "source": [
    "#loss function 정의\n",
    "def loss_function():\n",
    "#tf.nn.softmax_cross_entropy_with_logits을 활용하여 categorical cross entropy를 구현\n",
    "  z= tf.matmul(mnist_train,w)+b\n",
    "  softmax_result = tf.nn.softmax_cross_entropy_with_logits(logits=z,labels=train_labels)\n",
    "  cost = tf.reduce_mean(softmax_result)  \n",
    "  return cost\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01) #보편적으로 가장 많이 사용하는 adam optimizer 활용\n",
    "for step in range(10000): #train\n",
    "    cost_val=optimizer.minimize(loss_function, var_list=[w,b])\n",
    "    if step % 100 == 0:\n",
    "        print(step,\"loss_value:\", loss_function().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9123833\n"
     ]
    }
   ],
   "source": [
    "#최종 train accuracy 측정\n",
    "pred_y = tf.nn.softmax(tf.matmul(mnist_train,w)+b)\n",
    "argmax_y=tf.argmax(train_labels,1) #label 데이터의 argmax\n",
    "pred_final = tf.argmax(pred_y,1)#모델이 예측한 pred_y 데이터의 argmax\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred_final,argmax_y),dtype=tf.float32)) #train_accuracy\n",
    "print(\"Train accuracy:\", accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9014\n"
     ]
    }
   ],
   "source": [
    "#학습한 모델로 mnist test data predict\n",
    "test_pred= tf.nn.softmax(tf.matmul(mnist_test,w)+b)\n",
    "test_argmax_y=tf.argmax(test_labels ,1)\n",
    "test_pred_final= tf.argmax(test_pred,1) \n",
    "test_accuracy = tf.reduce_mean(tf.cast(tf.equal(test_pred_final,test_argmax_y),dtype=tf.float32)) #test_accuracy\n",
    "print(\"Test accuracy:\", test_accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
