{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mnist Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본적인 이미지 데이터 셋인 mnist 데이터를 tensorflow로 mlp를 구성하고 activation function으로 sigmoid가 아닌 relu를 사용하여 학습해보고자 한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tenssorflow==2.3 환경에서 구현  \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(mnist_train, mnist_train_label), (mnist_test, mnist_test_label) = mnist.load_data() #mnist data road\n",
    "mnist_train = tf.reshape(mnist_train, shape=(len(mnist_train), 28*28)) #mnist train data reshape\n",
    "mnist_train=tf.cast(mnist_train, tf.float32)\n",
    "mnist_test = tf.reshape(mnist_test, shape=(len(mnist_test), 28*28)) #mnist test data reshape\n",
    "mnist_test=tf.cast(mnist_test, tf.float32)\n",
    "\n",
    "#one hot encoding\n",
    "train_labels = to_categorical(mnist_train_label, 10)\n",
    "test_labels = to_categorical(mnist_test_label, 10) \n",
    "\n",
    "class_number = 10  \n",
    "feature_number=28*28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 수와 피쳐 수에 따라서 weigh와 bias 변수 설정\n",
    "w1 = tf.Variable(tf.random.uniform([feature_number,128]))\n",
    "b1 = tf.Variable(tf.random.uniform([128]))\n",
    "w2 = tf.Variable(tf.random.uniform([128,64]))\n",
    "b2 = tf.Variable(tf.random.uniform([64]))\n",
    "w3 = tf.Variable(tf.random.uniform([64,class_number]))\n",
    "b3 = tf.Variable(tf.random.uniform([class_number]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss_value: 1909604.9\n",
      "100 loss_value: 2357.4705\n",
      "200 loss_value: 893.8078\n",
      "300 loss_value: 580.6236\n",
      "400 loss_value: 419.74896\n",
      "500 loss_value: 319.6837\n",
      "600 loss_value: 250.48463\n",
      "700 loss_value: 1797.4695\n",
      "800 loss_value: 601.4719\n",
      "900 loss_value: 401.66052\n",
      "1000 loss_value: 298.73785\n",
      "1100 loss_value: 231.39034\n",
      "1200 loss_value: 182.96294\n",
      "1300 loss_value: 147.5794\n",
      "1400 loss_value: 120.801636\n",
      "1500 loss_value: 99.572716\n",
      "1600 loss_value: 82.69855\n",
      "1700 loss_value: 69.0089\n",
      "1800 loss_value: 57.830204\n",
      "1900 loss_value: 48.38575\n",
      "2000 loss_value: 44.766308\n",
      "2100 loss_value: 35.214066\n",
      "2200 loss_value: 845.8345\n",
      "2300 loss_value: 133.93362\n",
      "2400 loss_value: 71.02943\n",
      "2500 loss_value: 47.57078\n",
      "2600 loss_value: 35.19248\n",
      "2700 loss_value: 27.174444\n",
      "2800 loss_value: 21.680237\n",
      "2900 loss_value: 17.597069\n",
      "3000 loss_value: 14.448602\n",
      "3100 loss_value: 11.971116\n",
      "3200 loss_value: 9.961254\n",
      "3300 loss_value: 8.32881\n",
      "3400 loss_value: 6.990456\n",
      "3500 loss_value: 5.885799\n",
      "3600 loss_value: 4.960615\n",
      "3700 loss_value: 4.1655474\n",
      "3800 loss_value: 3.5078669\n",
      "3900 loss_value: 3.0272856\n",
      "4000 loss_value: 2.4162478\n",
      "4100 loss_value: 1.9784907\n",
      "4200 loss_value: 1.5992315\n",
      "4300 loss_value: 1.3131919\n",
      "4400 loss_value: 1.0361167\n",
      "4500 loss_value: 0.84685314\n",
      "4600 loss_value: 0.7216056\n",
      "4700 loss_value: 0.54161197\n",
      "4800 loss_value: 0.43145442\n",
      "4900 loss_value: 0.49253547\n",
      "5000 loss_value: 0.2470139\n",
      "5100 loss_value: 0.19788773\n",
      "5200 loss_value: 0.13454145\n",
      "5300 loss_value: 0.08636787\n",
      "5400 loss_value: 0.059059735\n",
      "5500 loss_value: 0.032930024\n",
      "5600 loss_value: 0.028154643\n",
      "5700 loss_value: 0.0044477573\n",
      "5800 loss_value: 0.0022124604\n",
      "5900 loss_value: 0.0012989822\n",
      "6000 loss_value: 0.00019063095\n",
      "6100 loss_value: 0.00026786796\n",
      "6200 loss_value: 0.0002719356\n",
      "6300 loss_value: 0.00020253757\n",
      "6400 loss_value: 7.449568e-05\n",
      "6500 loss_value: 6.586156e-05\n",
      "6600 loss_value: 0.0006057997\n",
      "6700 loss_value: 2.9274857e-05\n",
      "6800 loss_value: 4.2869495e-05\n",
      "6900 loss_value: 0.0026430949\n",
      "7000 loss_value: 7.2299176e-06\n",
      "7100 loss_value: 1.62922e-05\n",
      "7200 loss_value: 1.2103929e-05\n",
      "7300 loss_value: 1.7287366e-05\n",
      "7400 loss_value: 8.604692e-06\n",
      "7500 loss_value: 5.1818065e-06\n",
      "7600 loss_value: 6.057926e-06\n",
      "7700 loss_value: 1.9959452e-06\n",
      "7800 loss_value: 1.344226e-05\n",
      "7900 loss_value: 2.5572663e-06\n",
      "8000 loss_value: 5.685419e-07\n",
      "8100 loss_value: 3.1452976e-07\n",
      "8200 loss_value: 5.6667636e-07\n",
      "8300 loss_value: 2.965464e-07\n",
      "8400 loss_value: 6.9362477e-07\n",
      "8500 loss_value: 8.123252e-06\n",
      "8600 loss_value: 3.3682113e-07\n",
      "8700 loss_value: 1.7505364e-06\n",
      "8800 loss_value: 3.932873e-07\n",
      "8900 loss_value: 1.017104e-06\n",
      "9000 loss_value: 2.4479883e-07\n",
      "9100 loss_value: 2.4219057e-07\n",
      "9200 loss_value: 1.3910677e-07\n",
      "9300 loss_value: 1.9165514e-06\n",
      "9400 loss_value: 3.4426055e-06\n",
      "9500 loss_value: 1.7835971e-05\n",
      "9600 loss_value: 7.252985e-07\n",
      "9700 loss_value: 2.0251026e-07\n",
      "9800 loss_value: 1.1122449e-07\n",
      "9900 loss_value: 1.8467865e-07\n"
     ]
    }
   ],
   "source": [
    "#loss function 정의\n",
    "def loss_function():\n",
    "#tf.nn.softmax_cross_entropy_with_logits을 활용하여 categorical cross entropy를 구현\n",
    "  layer1= tf.nn.relu(tf.matmul(mnist_train,w1)+b1)\n",
    "  layer2 = tf.nn.relu(tf.matmul(layer1, w2) + b2)\n",
    "  layer3= tf.matmul(layer2,w3)+b3\n",
    "  softmax_result = tf.nn.softmax_cross_entropy_with_logits(logits=layer3,labels=train_labels)\n",
    "  cost = tf.reduce_mean(softmax_result)  \n",
    "  return cost\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate=0.01) #보편적으로 가장 많이 사용하는 adam optimizer 활용\n",
    "for step in range(10000): #train\n",
    "    cost_val=optimizer.minimize(loss_function, var_list=[w1,w2,w3,b1,b2,b3])\n",
    "    if step % 100 == 0:\n",
    "        print(step,\"loss_value:\", loss_function().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#최종 train accuracy 측정\n",
    "layer1= tf.nn.relu(tf.matmul(mnist_train,w1)+b1)\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, w2) + b2)\n",
    "pred_y = tf.nn.softmax(tf.matmul(layer2,w3)+b3)\n",
    "argmax_y=tf.argmax(train_labels,1) #label 데이터의 argmax\n",
    "pred_final = tf.argmax(pred_y,1)#모델이 예측한 pred_y 데이터의 argmax\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(pred_final,argmax_y),dtype=tf.float32)) #train_accuracy\n",
    "print(\"Train accuracy:\", accuracy.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.944\n"
     ]
    }
   ],
   "source": [
    "#학습한 모델로 mnist test data predict\n",
    "layer1= tf.nn.relu(tf.matmul(mnist_test,w1)+b1)\n",
    "layer2 = tf.nn.relu(tf.matmul(layer1, w2) + b2)\n",
    "test_pred= tf.nn.softmax(tf.matmul(layer2,w3)+b3)\n",
    "test_argmax_y=tf.argmax(test_labels ,1)\n",
    "test_pred_final= tf.argmax(test_pred,1) \n",
    "test_accuracy = tf.reduce_mean(tf.cast(tf.equal(test_pred_final,test_argmax_y),dtype=tf.float32)) #test_accuracy\n",
    "print(\"Test accuracy:\", test_accuracy.numpy()) "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
