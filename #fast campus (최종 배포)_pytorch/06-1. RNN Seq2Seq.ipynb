{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06-1. RNN Seq2Seq.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPNXODH2IWKKG/rjkff06iT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2lRrRsp4e1a6"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","import os\n","import sys\n","from datetime import datetime\n","\n","drive_project_root = \"/content/drive/MyDrive/#fastcampus\"\n","sys.path.append(drive_project_root)\n","!pip install -r \"/content/drive/MyDrive/#fastcampus/requirements.txt\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y2gozTx8e_5Z"},"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n","  print('and then re-execute this cell.')\n","else:\n","  print(gpu_info)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cy5EQxBhg1Ot"},"source":["# For data loading.\n","from typing import List\n","from typing import Dict\n","from typing import Union\n","from typing import Any\n","from typing import Optional\n","from typing import Iterable\n","from abc import abstractmethod\n","from abc import ABC\n","from datetime import datetime\n","from functools import partial\n","from collections import Counter\n","from collections import OrderedDict\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.nn.utils.rnn import pad_sequence\n","from torch.utils.data import DataLoader\n","import pytorch_lightning as pl\n","from pprint import pprint\n","\n","from torchtext import data\n","from torchtext import datasets\n","from torchtext.datasets import Multi30k\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.data.functional import to_map_style_dataset\n","from torchtext.vocab import Vocab, build_vocab_from_iterator, vocab\n","import spacy\n","\n","# For configuration\n","from omegaconf import DictConfig\n","from omegaconf import OmegaConf\n","import hydra\n","from hydra.core.config_store import ConfigStore\n","\n","# For logger\n","from torch.utils.tensorboard import SummaryWriter\n","import wandb\n","os.environ[\"WANDB_START_METHOD\"]=\"thread\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8RN_AA0ZRoJO"},"source":["from data_utils import dataset_split\n","from config_utils import flatten_dict\n","from config_utils import register_config\n","from config_utils import configure_optimizers_from_cfg\n","from config_utils import get_loggers\n","from config_utils import get_callbacks\n","from custom_math import softmax"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2rJQ1e5cjIaL"},"source":["# download eng/d data.\n","!python -m spacy download en\n","!python -m spacy download en_core_web_sm\n","!python -m spacy download de\n","!python -m spacy download de_core_news_sm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rvFCUvp0JVpi"},"source":["# practice data first go to dataconfig\n","\n","# data configs\n","data_spacy_de_en_cfg = {\n","    \"name\": \"spacy_de_en\",\n","    \"data_root\": os.path.join(os.getcwd(), \"data\"),\n","    \"tokenizer\": \"spacy\",\n","    \"src_lang\": \"de\",\n","    \"tgt_lang\": \"en\",\n","    \"src_index\": 0,\n","    \"tgt_index\": 1,\n","    \"vocab\": {\n","        \"special_symbol2index\": {\n","            # Define special symbols and indices\n","            # UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n","            # Make sure the tokens are in order of their indices to properly insert them in vocab\n","            '<unk>': 0,\n","            '<pad>': 1,\n","            '<bos>': 2,\n","            '<eos>': 3,\n","        },\n","        \"special_first\": True,\n","        \"min_freq\": 2\n","    }\n","}\n","\n","data_cfg = OmegaConf.create(data_spacy_de_en_cfg)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ahy7rqKvVHj6"},"source":["# get dataset\n","# data_root = os.path.join(os.getcwd(), \"data\")\n","\n","train_data, valid_data, test_data = Multi30k(data_cfg.data_root)\n","\n","test_data = to_map_style_dataset(test_data)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YTTXS5ptVE01"},"source":["# Create source and target language tokenizer. Make sure to install the dependencies.\n","# pip install -U spacy\n","# python -m spacy download en_core_web_sm\n","# python -m spacy download de_core_news_sm\n","\n","def get_token_transform(data_cfg: DictConfig) -> dict:\n","    token_transform = {}\n","    token_transform[data_cfg.src_lang] = get_tokenizer(\n","        data_cfg.tokenizer, language=data_cfg.src_lang\n","    )\n","    token_transform[data_cfg.tgt_lang] = get_tokenizer(\n","        data_cfg.tokenizer, language=data_cfg.tgt_lang\n","    )\n","    return token_transform\n","\n","token_transform = get_token_transform(data_cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DZTKeabmTPI2"},"source":["# helper function to yield list of tokens\n","def yield_tokens(\n","    data_iter: Iterable, lang: str, lang2index: Dict[str, int]\n",") -> List[str]:\n","\n","    for data_sample in data_iter:\n","        yield token_transform[lang](data_sample[lang2index[lang]])\n","\n","def get_vocab_transform(data_cfg: DictConfig) -> dict:\n","    vocab_transform = {}\n","    for ln in [data_cfg.src_lang, data_cfg.tgt_lang]:\n","        # Training data Iterator\n","        train_iter = Multi30k(\n","            split='train', language_pair=(data_cfg.src_lang, data_cfg.tgt_lang)\n","        )\n","        # Create torchtext's Vocab object\n","        vocab_transform[ln] = build_vocab_from_iterator(\n","            yield_tokens(\n","                train_iter,\n","                ln,\n","                {\n","                    data_cfg.src_lang: data_cfg.src_index,\n","                    data_cfg.tgt_lang: data_cfg.tgt_index\n","                }\n","            ),\n","            min_freq=data_cfg.vocab.min_freq,\n","            specials=list(data_cfg.vocab.special_symbol2index.keys()),\n","            special_first=data_cfg.vocab.special_first,\n","        )\n","\n","    # Set UNK_IDX as the default index. This index is returned when the token is not found.\n","    # If not set, it throws RuntimeError when the queried token is not found in the Vocabulary.\n","    for ln in [data_cfg.src_lang, data_cfg.tgt_lang]:\n","        vocab_transform[ln].set_default_index(\n","            data_cfg.vocab.special_symbol2index[\"<unk>\"]\n","        )\n","    return vocab_transform\n","\n","vocab_transform = get_vocab_transform(data_cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"495ATXpjVaNM"},"source":["print(vocab_transform[\"de\"][\"<unk>\"])\n","print(vocab_transform[\"en\"][\"<unk>\"])\n","print(vocab_transform[\"en\"][\"hello\"], vocab_transform[\"en\"][\"world\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5IhDtDo2S2gq"},"source":["# helper function to club together sequential operations\n","def sequential_transforms(*transforms):\n","    def func(txt_input):\n","        for transform in transforms:\n","            txt_input = transform(txt_input)\n","        return txt_input\n","    return func\n","\n","# function to add BOS/EOS and create tensor for input sequence indices\n","def tensor_transform(token_ids: List[int], bos_index: int, eos_index: int):\n","    return torch.cat((torch.tensor([bos_index]),\n","                      torch.tensor(token_ids),\n","                      torch.tensor([eos_index])))\n","    \n","# src and tgt language text transforms to convert raw strings into tensors indices\n","def get_text_transform(data_cfg):\n","    text_transform = {}\n","    for ln in [data_cfg.src_lang, data_cfg.tgt_lang]:\n","        text_transform[ln] = sequential_transforms(\n","            token_transform[ln], #Tokenization\n","            vocab_transform[ln], #Numericalization\n","            partial(\n","                tensor_transform,\n","                bos_index=data_cfg.vocab.special_symbol2index[\"<bos>\"],\n","                eos_index=data_cfg.vocab.special_symbol2index[\"<eos>\"],\n","            )\n","        ) # Add BOS/EOS and create tensor\n","    return text_transform\n","\n","text_transform = get_text_transform(data_cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kfrgrCnKVZJ9"},"source":["print(text_transform[\"en\"](\"hello\"))\n","print(text_transform[\"en\"](\"hello,\"))\n","print(text_transform[\"en\"](\"hello, how\"))\n","print(text_transform[\"en\"](\"hello, how are you ?\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UzuZ4z41T9vi"},"source":["# function to collate data samples into batch tesors\n","def collate_fn(batch, data_cfg: DictConfig):\n","    src_batch, tgt_batch = [], []\n","    for src_sample, tgt_sample in batch:\n","        src_batch.append(text_transform[data_cfg.src_lang](src_sample.rstrip(\"\\n\")))\n","        tgt_batch.append(text_transform[data_cfg.tgt_lang](tgt_sample.rstrip(\"\\n\")))\n","\n","    src_batch = pad_sequence(src_batch, padding_value=data_cfg.vocab.special_symbol2index[\"<pad>\"])\n","    tgt_batch = pad_sequence(tgt_batch, padding_value=data_cfg.vocab.special_symbol2index[\"<pad>\"])\n","    return src_batch, tgt_batch\n","\n","def get_collate_fn(cfg: DictConfig):\n","    return partial(collate_fn, data_cfg=cfg.data)\n","\n","def get_multi30k_dataloader(\n","    split_mode: str, language_pair, batch_size: int, collate_fn\n","):\n","    iter = Multi30k(split=split_mode, language_pair=language_pair)\n","    dataset = to_map_style_dataset(iter)\n","    dataloader = torch.utils.data.DataLoader(\n","        dataset, batch_size=batch_size, collate_fn=collate_fn\n","    )\n","    return dataloader\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lH0CHCr7hh-8"},"source":["SEED = 1234\n","\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKy2UW9qWa__"},"source":["def _text_postprocessing(res: List[str]) -> str:\n","    if \"<eos>\" in res:\n","        res = res[:res.index(\"<eos>\")]\n","    if \"<pad>\" in res:\n","        res = res[:res.index(\"<pad>\")]\n","    res = \" \".join(res).replace(\"<bos>\", \"\")\n","    return res\n","\n","class BaseTranslateLightningModule(pl.LightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__()\n","        self.cfg = cfg\n","        self.loss_function = torch.nn.CrossEntropyLoss(\n","            ignore_index=cfg.data.vocab.special_symbol2index[\"<pad>\"]\n","        )\n","    \n","    def configure_optimizers(self):\n","        self._optimizers, self._schedulers = configure_optimizers_from_cfg(\n","            self.cfg, self\n","        )\n","        return self._optimizers, self._schedulers\n","    \n","    @abstractmethod\n","    def forward(x):\n","        raise NotImplementedError()\n","    \n","    def _forward(self, src, tgt, mode: str, teacher_forcing_ratio: float = 0.5):\n","        \n","        assert mode in [\"train\", \"val\", \"test\"]\n","        \n","        # get predictions\n","        tgt_inputs = tgt[:-1, :] # delete ends...\n","        outputs = self(src, tgt_inputs, teacher_forcing_ratio=teacher_forcing_ratio)\n","        tgt_outputs = tgt[1:, :]  # remove start tokens..\n","\n","        loss = self.loss_function(\n","            outputs.reshape(-1, outputs.shape[-1]),\n","            tgt_outputs.reshape(-1)\n","        )\n","\n","        logs_detail = {\n","            f\"{mode}_src\": src,\n","            f\"{mode}_tgt\": tgt,\n","            f\"{mode}_results\": outputs,\n","        }\n","\n","        if mode in [\"val\", \"test\"]:\n","            _, tgt_results = torch.max(outputs, dim=2)\n","        \n","            src_texts = []\n","            tgt_texts = []\n","            res_texts = []\n","\n","            for src_i in torch.transpose(src, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.src_lang].lookup_tokens(src_i)\n","                src_texts.append(_text_postprocessing(res))\n","            \n","            for tgt_i in torch.transpose(tgt, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[self.cfg.data.tgt_lang].lookup_tokens(tgt_i)\n","                tgt_texts.append(_text_postprocessing(res))\n","\n","            for tgt_res_i in torch.transpose(tgt_results, 0, 1).detach().cpu().numpy().tolist():\n","                res = vocab_transform[cfg.data.tgt_lang].lookup_tokens(tgt_res_i)\n","                res_texts.append(_text_postprocessing(res))\n","\n","            text_result_summary = {\n","                f\"{mode}_src_text\": src_texts,\n","                f\"{mode}_tgt_text\": tgt_texts,\n","                f\"{mode}_results_text\": res_texts,\n","            }\n","            print(f\"{self.global_step} step: \\n src_text: {src_texts[0]}, \\n tgt_text: {tgt_texts[0]}, \\n result_text:{res_texts[0]}\")\n","            logs_detail.update(text_result_summary)\n","\n","        return {f\"{mode}_loss\": loss}, logs_detail\n","    \n","    def training_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","\n","        logs, logs_detail = self._forward(src, tgt, \"train\", self.cfg.model.teacher_forcing_ratio)\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"train_loss\"]\n","        return logs\n","    \n","    def validation_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, logs_detail = self._forward(src, tgt, \"val\", 0.0)\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"val_loss\"]\n","        logs.update(logs_detail)\n","        \n","        return logs\n","    \n","    def test_step(self, batch, batch_idx):\n","        src, tgt = batch[0], batch[1]\n","        logs, logs_detail = self._forward(images, labels, \"test\", 0.0)\n","        self.log_dict(logs)\n","        logs[\"loss\"] = logs[\"test_loss\"]\n","        logs.update(logs_detail)\n","        # wandb_logger, tensorboard_logger = self.logger.experiment\n","        # wandb_logger.log(logs_detail)\n","        # self.log_dict(logs)\n","        return logs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FbnX8ZJZibpy"},"source":["# weight initialization\n","def init_weights(model: Union[nn.Module, pl.LightningModule]):\n","    for name, param in model.named_parameters():\n","        nn.init.uniform_(param.data, -0.08, 0.08)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eCTD0MaS_vVN"},"source":["class LSTMEncoder(nn.Module):\n","    def __init__(\n","        self,\n","        input_dim: int,\n","        embed_dim: int,\n","        hidden_dim: int,\n","        n_layers: int,\n","        dropout: float\n","    ):\n","        super().__init__()\n","        \n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        \n","        self.embedding = nn.Embedding(input_dim, embed_dim)\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout = dropout)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.apply(init_weights)\n","        \n","    def forward(self, src):\n","        # src = [src len, batch size]\n","        embedded = self.dropout(self.embedding(src))\n","        \n","        # embedded = [src len, batch size, emb dim]\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","        \n","        # outputs = [src len, batch size, hid dim * n directions]\n","        # hidden = [n layers * n directions, batch size, hid dim]\n","        # cell = [n layers * n directions, batch size, hid dim]\n","        \n","        # outputs are always from the top hidden layer\n","        return hidden, cell\n","\n","\n","class LSTMDecoder(nn.Module):\n","    def __init__(\n","        self,\n","        output_dim: int,\n","        embed_dim: int,\n","        hidden_dim: int,\n","        n_layers: int,\n","        dropout: float,\n","    ):\n","        super().__init__()\n","        \n","        self.output_dim = output_dim\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","        \n","        self.embedding = nn.Embedding(output_dim, embed_dim)\n","        self.rnn = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout = dropout)\n","        self.fc_out = nn.Linear(hidden_dim, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, input, hidden, cell):        \n","        # input = [batch size]\n","        # hidden = [n layers * n directions, batch size, hid dim]\n","        # cell = [n layers * n directions, batch size, hid dim]\n","\n","        # n directions in the decoder will both always be 1, therefore:\n","        # hidden = [n layers, batch size, hid dim]\n","        # context = [n layers, batch size, hid dim]\n","        \n","        input = input.unsqueeze(0)\n","        \n","        # input = [1, batch size]\n","        embedded = self.dropout(self.embedding(input))\n","        \n","        # embedded = [1, batch size, emb dim]\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","        \n","        # output = [seq len, batch size, hid dim * n directions]\n","        # hidden = [n layers * n directions, batch size, hid dim]\n","        # cell = [n layers * n directions, batch size, hid dim]\n","        \n","        # seq len and n directions will always be 1 in the decoder, therefore:\n","        # output = [1, batch size, hid dim]\n","        # hidden = [n layers, batch size, hid dim]\n","        # cell = [n layers, batch size, hid dim]\n","        \n","        prediction = self.fc_out(output.squeeze(0))\n","        # prediction = [batch size, output dim]\n","\n","        return prediction, hidden, cell\n","\n","\n","class LSTMSeq2Seq(BaseTranslateLightningModule):\n","    def __init__(self, cfg: DictConfig):\n","        super().__init__(cfg)\n","        # encoder, decoder, device\n","        \n","        self.encoder = LSTMEncoder(**cfg.model.enc)\n","        self.decoder = LSTMDecoder(**cfg.model.dec)\n","        # self.device = device\n","        \n","        assert self.encoder.hidden_dim == self.decoder.hidden_dim, \\\n","            \"Hidden dimensions of encoder and decoder must be equal!\"\n","        assert self.encoder.n_layers == self.decoder.n_layers, \\\n","            \"Encoder and decoder must have equal number of layers!\"\n","        \n","        self.apply(init_weights)\n","        \n","    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n","        \n","        # src = [src len, batch size]\n","        # trg = [trg len, batch size]\n","        # teacher_forcing_ratio is probability to use teacher forcing\n","        # e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n","        \n","        batch_size = trg.shape[1]\n","        trg_len = trg.shape[0]\n","        trg_vocab_size = self.decoder.output_dim\n","        \n","        #tensor to store decoder outputs\n","        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n","        \n","        #last hidden state of the encoder is used as the initial hidden state of the decoder\n","        hidden, cell = self.encoder(src)\n","        \n","        #first input to the decoder is the <sos> tokens\n","        input = trg[0,:]\n","        \n","        for t in range(1, trg_len):\n","            \n","            #insert input token embedding, previous hidden and previous cell states\n","            #receive output tensor (predictions) and new hidden and cell states\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","            \n","            #place predictions in a tensor holding predictions for each token\n","            outputs[t] = output\n","            \n","            #decide if we are going to use teacher forcing or not\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            \n","            #get the highest predicted token from our predictions\n","            top1 = output.argmax(1) \n","            \n","            #if teacher forcing, use actual next token as next input\n","            #if not, use predicted token\n","            input = trg[t] if teacher_force else top1\n","        \n","        return outputs\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ydyN6EmnQfLT"},"source":["\n","# data configs\n","data_spacy_de_en_cfg = {\n","    \"name\": \"spacy_de_en\",\n","    \"data_root\": os.path.join(os.getcwd(), \"data\"),\n","    \"tokenizer\": \"spacy\",\n","    \"src_lang\": \"de\",\n","    \"tgt_lang\": \"en\",\n","    \"src_index\": 0,\n","    \"tgt_index\": 1,\n","    \"vocab\": {\n","        \"special_symbol2index\": {\n","            # Define special symbols and indices\n","            # UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n","            # Make sure the tokens are in order of their indices to properly insert them in vocab\n","            '<unk>': 0,\n","            '<pad>': 1,\n","            '<bos>': 2,\n","            '<eos>': 3,\n","        },\n","        \"special_first\": True,\n","        \"min_freq\": 2\n","    }\n","}\n","\n","data_cfg = OmegaConf.create(data_spacy_de_en_cfg)\n","\n","# get dataset\n","# data_root = os.path.join(os.getcwd(), \"data\")\n","train_data, valid_data, test_data = Multi30k(data_cfg.data_root)\n","\n","token_transform = get_token_transform(data_cfg)\n","vocab_transform = get_vocab_transform(data_cfg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xIjxSubTLAKb"},"source":["# model configs\n","model_translate_lstm_seq2seq_cfg = {\n","    \"name\": \"LSTMSeq2Seq\",\n","    \"out_dim\": len(vocab_transform[data_cfg.src_lang]),\n","    \"enc\": {\n","        \"input_dim\": len(vocab_transform[data_cfg.src_lang]),\n","        \"embed_dim\": 256,\n","        \"hidden_dim\": 256,\n","        \"n_layers\": 2,\n","        \"dropout\": 0.5,\n","    },\n","    \"dec\": {\n","        \"output_dim\": len(vocab_transform[data_cfg.tgt_lang]),\n","        \"embed_dim\": 256,\n","        \"hidden_dim\": 256,\n","        \"n_layers\": 2,\n","        \"dropout\": 0.5,\n","    },\n","    \"teacher_forcing_ratio\": 0.5\n","}\n","\n","# optimizer configs\n","opt_cfg = {\n","    \"optimizers\": [\n","        {\n","            \"name\": \"RAdam\",\n","            \"kwargs\": {\n","                \"lr\": 1e-3,\n","            }\n","        }\n","    ],\n","    \"lr_schedulers\": [\n","        {\n","            \"name\": None,\n","            \"kwargs\": {\n","                \"warmup_end_steps\": 1000\n","            }\n","        },\n","    ]\n","}\n","\n","_merged_cfg_presets = {\n","    \"LSTM_seq2seq_de_en_translate\": {\n","        \"opt\": opt_cfg,\n","        \"data\": data_spacy_de_en_cfg,\n","        \"model\": model_translate_lstm_seq2seq_cfg,\n","    },\n","}\n","\n","# clear config instance first\n","hydra.core.global_hydra.GlobalHydra.instance().clear()\n","\n","# register preset configs\n","register_config(_merged_cfg_presets)\n","\n","# initialize & make config\n","## select mode here ##\n","# .................. #\n","hydra.initialize(config_path=None)\n","cfg = hydra.compose(\"LSTM_seq2seq_de_en_translate\")\n","\n","# override some cfg\n","run_name = f\"{datetime.now().isoformat(timespec='seconds')}-{cfg.model.name}-{cfg.data.name}\"\n","\n","# Define other train configs & log_configs \n","# Merge configs into one & register it to Hydra.\n","project_root_dir = os.path.join(\n","    drive_project_root, \"runs\", \"de_en_translate\"\n",")\n","save_dir = os.path.join(project_root_dir, run_name)\n","run_root_dir = os.path.join(project_root_dir, run_name)\n","\n","train_cfg = {\n","    \"train_batch_size\": 128,\n","    \"val_batch_size\": 32,\n","    \"test_batch_size\": 32,\n","    \"train_val_split\": [0.9, 0.1],\n","    \"run_root_dir\": run_root_dir,\n","    \"trainer_kwargs\": {\n","        \"accelerator\": \"dp\",\n","        \"gpus\": \"0\",\n","        \"max_epochs\": 50,\n","        \"val_check_interval\": 1.0,\n","        \"log_every_n_steps\": 100,\n","        \"flush_logs_every_n_steps\": 100,\n","    }\n","}\n","\n","# logger config\n","log_cfg = {\n","    \"loggers\": {\n","        \"WandbLogger\": {\n","            \"project\": \"fastcampus_de_en_translate_tutorials\",\n","            \"name\": run_name,\n","            \"tags\": [\"fastcampus_de_en_translate_tutorials\"],\n","            \"save_dir\": run_root_dir,\n","        },\n","        \"TensorBoardLogger\": {\n","            \"save_dir\": project_root_dir,\n","            \"name\": run_name,\n","            \"log_graph\": True,\n","        }\n","    },\n","    \"callbacks\": {\n","        \"ModelCheckpoint\": {\n","            \"save_top_k\": 3,\n","            \"monitor\": \"val_loss\",\n","            \"mode\": \"min\",\n","            \"verbose\": True,\n","            \"dirpath\": os.path.join(run_root_dir, \"weights\"),\n","            \"filename\": \"{epoch}-{val_loss:.3f}\",\n","        },\n","        \"EarlyStopping\": {\n","            \"monitor\": \"val_loss\",\n","            \"mode\": \"min\",\n","            \"patience\": 3,\n","            \"verbose\": True,\n","        }\n","    }\n","}\n","\n","# unlock config & set train_cfg & log_cfg\n","OmegaConf.set_struct(cfg, False)\n","cfg.train = train_cfg \n","cfg.log = log_cfg\n","\n","# lock config\n","OmegaConf.set_struct(cfg, True)\n","print(OmegaConf.to_yaml(cfg))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5YG__y9MXx2N"},"source":["# dataloader def\n","train_dataloader = get_multi30k_dataloader(\n","    \"train\",\n","    (cfg.data.src_lang, cfg.data.tgt_lang),\n","    cfg.train.train_batch_size,\n","    collate_fn=get_collate_fn(cfg)\n",")\n","val_dataloader = get_multi30k_dataloader(\n","    \"valid\",\n","    (cfg.data.src_lang, cfg.data.tgt_lang),\n","    cfg.train.val_batch_size,\n","    collate_fn=get_collate_fn(cfg)\n",")\n","test_dataloader = get_multi30k_dataloader(\n","    \"test\",\n","    (cfg.data.src_lang, cfg.data.tgt_lang),\n","    cfg.train.test_batch_size,\n","    collate_fn=get_collate_fn(cfg)\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L_77rnkHeMiq"},"source":["# model definition\n","def get_pl_model(cfg: DictConfig, checkpoint_path: Optional[str] = None):\n","    if cfg.model.name == \"LSTMSeq2Seq\":\n","        model = LSTMSeq2Seq(cfg)\n","    else:\n","        raise NotImplementedError(\"not implemented model\")\n","    \n","    if checkpoint_path is not None:\n","        model = model.load_from_checkpoint(checkpoint_path=checkpoint_path)\n","    return model\n","\n","model = None\n","model = get_pl_model(cfg)\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FWOEyvQNhG4O"},"source":["# pytorch-lightning trainer def\n","logger = get_loggers(cfg)\n","callbacks = get_callbacks(cfg)\n","\n","trainer = pl.Trainer(\n","    callbacks=callbacks,\n","    logger=logger,\n","    default_root_dir=cfg.train.run_root_dir,\n","    num_sanity_val_steps=2,\n","    **cfg.train.trainer_kwargs,\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AkhYbOhBbDC8"},"source":["trainer.fit(model, train_dataloader, val_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AymuOmTSeUcz"},"source":["def post_evaluate(cfg, model, src, device: str = \"cpu\"):\n","    model.eval()\n","\n","    src_texts = []\n","    tgt_texts = []\n","    res_texts = []\n","    for i, (src, tgt) in enumerate(test_dataloader):\n","        src = src.to(device)\n","        tgt = tgt.to(device)\n","\n","        logits = model(src, tgt, teacher_forcing_ratio=0.0)\n","        _, tgt_results = torch.max(logits, dim=2)\n","        \n","        # Convert to [seq_size, batch, ...] -> [batch, seq_size, ...]\n","        src = torch.transpose(src, 0, 1)\n","        tgt = torch.transpose(tgt, 0, 1)\n","        tgt_results = torch.transpose(tgt_results, 0, 1)\n","\n","        for src_i in src.detach().cpu().numpy().tolist():\n","            res = vocab_transform[cfg.data.src_lang].lookup_tokens(src_i)\n","            if \"<eos>\" in res:\n","                res = res[:res.index(\"<eos>\")]\n","            if \"<pad>\" in res:\n","                res = res[:res.index(\"<pad>\")]\n","            res = \" \".join(res).replace(\"<bos>\", \"\")\n","            src_texts.append(res)\n","        \n","        for tgt_i in tgt.detach().cpu().numpy().tolist():\n","            res = vocab_transform[cfg.data.tgt_lang].lookup_tokens(tgt_i)\n","            if \"<eos>\" in res:\n","                res = res[:res.index(\"<eos>\")]\n","            if \"<pad>\" in res:\n","                res = res[:res.index(\"<pad>\")]\n","            res = \" \".join(res).replace(\"<bos>\", \"\")\n","            tgt_texts.append(res)\n","        \n","        print(tgt_results)\n","        for tgt_res_i in tgt_results.detach().cpu().numpy().tolist():\n","            res = vocab_transform[cfg.data.tgt_lang].lookup_tokens(tgt_res_i)\n","\n","            if \"<eos>\" in res:\n","                res = res[:res.index(\"<eos>\")]\n","            if \"<pad>\" in res:\n","                res = res[:res.index(\"<pad>\")]\n","            \n","            res = \" \".join(res).replace(\"<bos>\", \"\")\n","            res_texts.append(res)\n","        \n","        # # print(src_texts[-1])\n","        # # print(tgt_texts[-1])\n","        # # print(res_texts[-1])\n","\n","        # print('check::', res_texts)\n","        # from pprint import pprint\n","        # pprint(list(zip(src_texts, tgt_texts, res_texts)))\n","        # break\n","\n","    assert len(src_texts) == len(tgt_texts) and len(tgt_texts) == len(res_texts)\n","    return list(zip(src_texts, tgt_texts, res_texts))\n","\n","\n","checkpoint_path = os.path.join(project_root_dir, \"2021-08-16T10:06:43-LSTMSeq2Seq-spacy_de_en\", \"weights\", \"epoch=22-val_loss=3.856-val_acc=0.000.ckpt\")\n","model = model.load_from_checkpoint(cfg=cfg, checkpoint_path=checkpoint_path)\n","evaluate(cfg, model, test_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rIMNj_Kd0Pyt"},"source":["\n","def train_epoch(model, optimizer):\n","    model.train()\n","    losses = 0\n","    train_iter = Multi30k(\n","        split='train', language_pair=(SRC_LANG, TGT_LANG)\n","    )\n","    train_dataloader = torch.utils.data.DataLoader(\n","        train_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn\n","    )\n","\n","    for src, tgt in train_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[:-1, :]\n","\n","        logits = model(src, tgt_input)\n","\n","        # src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        # logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        optimizer.zero_grad()\n","\n","        tgt_out = tgt[1:, :]\n","\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        loss.backward()\n","\n","        optimizer.step()\n","        losses += loss.item()\n","\n","    return losses / len(train_dataloader)\n","\n","def evaluate(model):\n","    model.eval()\n","    losses = 0\n","\n","    val_iter = Multi30k(split='valid', language_pair=(SRC_LANG, TGT_LANG))\n","    val_dataloader = DataLoader(\n","        val_iter, batch_size=BATCH_SIZE, collate_fn=collate_fn\n","    )\n","\n","    for src, tgt in val_dataloader:\n","        src = src.to(DEVICE)\n","        tgt = tgt.to(DEVICE)\n","\n","        tgt_input = tgt[:-1, :]\n","\n","        # src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n","\n","        logits = model(src, tgt_input)\n","        # logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n","\n","        tgt_out = tgt[1:, :]\n","\n","        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n","        losses += loss.item()\n","\n","    return losses / len(val_dataloader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xqrjLlVLNPmU"},"source":["i = 0\n","for i, epoch in enumerate(tqdm(range(1, NUM_EPOCHS+1), position=0, leave=True, desc=f\"epoch_{i}:\")):\n","    start_time = timer()\n","    train_loss = train_epoch(model, optimizer)\n","    end_time = timer()\n","    val_loss = evaluate(model)\n","    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n","\n","    print(translate(model, \"Eine Gruppe von Menschen steht vor einem Iglu .\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XXX9DLBk1QHG"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FUkpQvVm1QV6"},"source":[""],"execution_count":null,"outputs":[]}]}